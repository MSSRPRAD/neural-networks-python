{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "c95YPBOuN0Vr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras as keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
      ],
      "metadata": {
        "id": "2G2A3cFsOKBk"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzDLrmWiOuZ_",
        "outputId": "c2eab8ac-d568-4aec-b0e3-2f9a242e35df"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(train_X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6m48XiGPFsY",
        "outputId": "69e25947-ef8f-47cb-c4b7-cb1db7c0b9d8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efnevcfJPTsl",
        "outputId": "d9678e95-f8ed-4113-abbe-f89c467eb02c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "28*28\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NNJeQjPQORg",
        "outputId": "ee29eeb0-3c72-49a8-d303-4b3522c5785b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "784"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = []\n",
        "for i in range(len(train_X)):\n",
        "  train.append(np.reshape(train_X[i], (1, 784)))\n",
        "\n",
        "x_train = np.array(train)\n",
        "y_train = train_y"
      ],
      "metadata": {
        "id": "jQqXnK1VQifh"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.reshape(train_y, (len(train_y)))"
      ],
      "metadata": {
        "id": "NuDZRkwAP4li"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "for i in range(len(x_train)):\n",
        "  for j in range(len(x_train[i][0])):\n",
        "    if x_train[i][0][j] > 0 :\n",
        "      x_train[i][0][j] = 1"
      ],
      "metadata": {
        "id": "RPfWcdqrZDO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0].shape"
      ],
      "metadata": {
        "id": "P-NLzn0fZoaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0]"
      ],
      "metadata": {
        "id": "7G9VDWrQSyLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input neurons = 784\n",
        "# Output neurons = 10"
      ],
      "metadata": {
        "id": "sVkiY1O0Ym8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0]"
      ],
      "metadata": {
        "id": "XvO_CYPZdfgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process y\n",
        "new_y_train = []\n",
        "for i in range(len(y_train)):\n",
        "  l = np.zeros((10))\n",
        "  l[y_train[i]]=1\n",
        "  new_y_train.append(l)\n",
        "\n",
        "y_train = np.array(new_y_train)\n"
      ],
      "metadata": {
        "id": "PhrIZ8ikcPQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "id": "Lf5_xDf5AmCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "ZLvPniEPAooh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will make a helper class for the difference activation functions"
      ],
      "metadata": {
        "id": "YOcc7sNJBGXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation:\n",
        "  def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "  def tanh(x):\n",
        "    return np.tanh(x)\n",
        "  def relu(x):\n",
        "    return np.maximum(x,0)\n",
        "  def sigmoid_der(x):\n",
        "    return x*(1-x)\n",
        "  def tanh_der(x):\n",
        "    return 1-np.power(np.tanh(x), 2)\n",
        "  def relu_der(x):\n",
        "    return np.where(x >= 0, 1, 0)"
      ],
      "metadata": {
        "id": "3MTEULMpBL04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper metrics class"
      ],
      "metadata": {
        "id": "EyrSXNEjBi3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Metrics:\n",
        "  def __init__(self, y_pred, y_test):\n",
        "    self.y_pred = y_pred\n",
        "    self.y_actual = y_test\n",
        "    self.len = len(y_pred)\n",
        "\n",
        "  def accuracy(self):\n",
        "    # Count the number of matching elements\n",
        "    equals = np.sum(np.array_equal(self.y_pred, self.y_actual))\n",
        "    return equals/self.len\n",
        "\n",
        "  def mean_square_error(self):\n",
        "    return np.mean(np.power(self.y_pred - self.y_actual, 2))\n",
        "  \n"
      ],
      "metadata": {
        "id": "Prq3eozzBgjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making the layers for the library\n",
        "\n",
        "It's properties are:\n",
        " 1. It takes an input from a previous layer \n",
        " 2. It gives and output to the next layer\n",
        " 3. It computes the output through forward propogation\n",
        " 4. It updates it's parameters through back propogation"
      ],
      "metadata": {
        "id": "_RIZb3xqFWhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "  def __init__(self):\n",
        "    self.input = None\n",
        "    self.output = None\n",
        "  \n",
        "  def foward_propogate(self, input):\n",
        "    raise NotImplemenetedError\n",
        "  \n",
        "  def backward_propogate(self, output_error, lr):\n",
        "    raise NotImplementedError"
      ],
      "metadata": {
        "id": "KKFcGu04FT82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us define a Dense Layer (where every neuron is connected to every other neuron in the next layer) which is a subclass of our Layer class."
      ],
      "metadata": {
        "id": "XYmILMaWGx1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseLayer(Layer):\n",
        "  # input_size = no of input neurons\n",
        "  # output_size = no of output neurons\n",
        "  def __init__(self, input_size, output_size, activation):\n",
        "    # weights is an array of input_size rows and output_size columns\n",
        "    # i-th row j-th column is the weight connecting i-th output neuron to j-th input neuron\n",
        "    self.weights = np.zeros(input_size, output_size)\n",
        "    # Apart from these there must be a bias neuron that is connected to all the output neurons\n",
        "    self.bias = np.zeros(1,output_size)\n",
        "    # Activation Function for this layer\n",
        "    self.activation = activation\n",
        "    # Input values, activation (weighted avg of input), outputs (activation function applied on activations), errors (of each neuron)\n",
        "    self.inputs = np.zeros(1, input_size)\n",
        "    self.activations = np.zeros(1, output_size)\n",
        "    self.outputs = np.zeros(1, output_size)\n",
        "    self.input_errors = np.zeros(1, input_size)\n",
        "\n",
        "  # Given the input calculate the outputs (using the weights)\n",
        "  # Input is a numpy array of 1 rows and input_size columns\n",
        "  # Output is a numpy array of 1 rows and output_size columns\n",
        "  def forward_propogate(self, inputs):\n",
        "    self.inputs = inputs\n",
        "    self.activations = np.dot(self.inputs, self.weights) + self.bias\n",
        "    ac = Activation()\n",
        "    match self.activation:\n",
        "      case \"relu\":\n",
        "        self.output = ac.relu(self.activation)\n",
        "      case \"sigmoid\":\n",
        "        self.output = ac.relu(self.activation)\n",
        "      case \"tanh\":\n",
        "        self.output = ac.relu(self.activation)\n",
        "    return self.output\n",
        "  \n",
        "  # output_errors is a numpy array with 1 rows and output_size columns\n",
        "  def backward_propogate(self, output_errors, lr):\n",
        "    ac = Activation()\n",
        "    # backpropogate the errors\n",
        "    match self.activation:\n",
        "      case \"relu\":\n",
        "        self.input_errors = ac.relu_der(self.activation)*(np.dot(self.weights, output_errors.T)).T\n",
        "      case \"sigmoid\":\n",
        "        self.input_errors = ac.sigmoid_der(self.activation)*(np.dot(self.weights, output_errors.T)).T\n",
        "      case \"tanh\":\n",
        "        self.input_errors = ac.tanh_der(self.activation)*(np.dot(self.weights, output_errors.T)).T\n",
        "    # Calculate the error in weights for this layer \n",
        "    weight_errors = np.dot(self.inputs.T, self.output_errors)\n",
        "    # Update the weights\n",
        "    self.weights -= lr*weight_errors\n",
        "    # Update the bias\n",
        "    self.bias -= lr*output_errors\n"
      ],
      "metadata": {
        "id": "LRUuPCFuGJg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Class\n",
        "\n",
        "Will have all our dense layers (and the initial layer and final layer)"
      ],
      "metadata": {
        "id": "RlQwQcvTqBC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self):\n",
        "    self.layers = []\n",
        "    self.loss = None\n",
        "\n",
        "  # Add a layer to our model\n",
        "  def add(self, layer):\n",
        "    self.layers.append(layer)\n",
        "    return\n",
        "  \n",
        "  # Predict Output (through forward propogation)\n",
        "  def predict(self, input_data):\n",
        "    y_pred = []\n",
        "    # Run the neural network over all the samples (stochastic gradient descent)\n",
        "    for i in range(len(input_data)):\n",
        "      # Forward propogate the values\n",
        "      output = input_data[i]\n",
        "      for layer in self.layers:\n",
        "        output = layer.forward_propogate(output)\n",
        "      y_pred.append(output)\n",
        "    return y_pred\n",
        "\n",
        "  # Train the network\n",
        "  def fit(self, x_train, y_train, epochs=10, lr=0.001):\n",
        "    self.loss = []\n",
        "    for i in range(epochs):\n",
        "      errors = 0\n",
        "      for j in range(len(x_train)):\n",
        "        # Forward propogate the values\n",
        "        output = x_train[i]\n",
        "        for layer in self.layers:\n",
        "          output = layer.forward_propogate(output)\n",
        "        errors = output - y_train[i]\n",
        "        # Store the loss\n",
        "        metrics = Metrics(output, y_train[i])\n",
        "        self.loss = metrics.mean_square_error()\n",
        "        # Backpropogate the errors\n",
        "        for layer in reversed(self.layers):\n",
        "          errors = layer.backward_propogate(errors, epochs, lr)"
      ],
      "metadata": {
        "id": "EduFpKRvqAab"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}